<?php include("header.htm"); ?>

<h5>Positions</h5>
<ul>

<li>Director of Engagement, <a class="urllink" href="https://www.monash.edu/it/dsai" rel="nofollow" target="_blank">
Department of Data Sciece and AI</a>, <a class="urllink" href="http://www.monash.edu.au" rel="nofollow" target="_blank">Monash University</a>, 2024-now </li> 
<li>Head of the Language and Speech Processing, <a class="urllink" href="https://openstream.ai/" rel="nofollow" target="_blank"> 
OpenStream AI</a>, 2022-now. </li>
<li>ARC Future Fellow and  Professor, <a class="urllink" href="https://www.monash.edu/it/dsai" rel="nofollow" target="_blank">
Department of Data Sciece and AI</a>, <a class="urllink" href="http://www.monash.edu.au" rel="nofollow" target="_blank">Monash University</a>, 2022-now</li>
<li>Director of the <a class="urllink" href="https://www.monash.edu/it/dsai/vision-language" rel="nofollow" target="_blank">Vision and Language Group</a>, 
<a class="urllink" href="https://www.monash.edu/it/dsai" rel="nofollow" target="_blank">
Department of Data Sciece and AI</a>, <a class="urllink" href="http://www.monash.edu.au" rel="nofollow" target="_blank">Monash University</a>, 2020-2024.
</li>

</ul>

<div class="vspace"></div><h5>Prospective Students</h5>
<p>
    <a class="urllink" href="../Propsective/propsective.html" rel="nofollow" target="_blank">
        For PhD application, please read this before you contact me.</a>
    To international undergrad students: Please email me only if you have your own funding for internship
    as I do not have any funding to support interns.
</p>

<div class="vspace"></div><h5>Research Areas</h5>
<ul>
    <li>Neuro-Symbolic AI Agents, e.g. integrating LLMs and symbolic methods for multi-modal agents that operate/learn by perception, planning, acting, and feeback</li>
    <li>Knowledge Graphs and LLMs, e.g. effective approaches for hybrid LLM-KG reasoning, certifying LLM knwoledge/reasoning</li>
    <li>Efficient and effective LLMs, e.g. better architectures, better decoding/inference, better training algorithms (long-context LLMs, KV-caching)</li>
    <li>Continual Learning, e.g. LLM knwoledge editing, retreival augmented generation (RAG)</li>
    <li>Reliable and Safe LLMs, e.g. alignment algorithms (RLHF), social-cultural norms, jailbreaking and defence mechanisms</li>
    <li>Multilingual and Multimodal Foundation Models, e.g. covering low-resource languages, speech/video/text/code modalities</li>
    <li>Real-life Applications, e.g. text/speech/document translation, question-answering, coding, dialogue systems</li>
</ul>

<div class="vspace"></div><h5>Earlier Research Areas</h5>
<ul>
    <li>Deep learning methods, particularly why they work and how to use them well.</li>
    <li>Structured prediction, particularly predicting complex linguistic structure.</li>
    <li>Discourse, semantics, syntax, and morphology; particularly for machine translation.</li>
    <li>Advanced data structures (eg succinct suffix trees) for large-scale NLP problem, such as language modelling.</li>
    <li>Learning with limited amounts of supervision and large amount of un-annotated data; learning across different domains of data.</li>
    <li>Learning and inference in probabilistic graphical models, particularly for NLP problems.</li>
    <li>Non-parametric Bayesian models, particularly for NLP.</li>
    <li>Reinforcement learning, Markov decision processes, and multi-armed bandit.</li>
</ul>

<div class="vspace"></div><h5>Recent Grants and Projects</h5>
<ul>
<li> <p> Aligning Large Language Models with Human Intention, EBay Research, 130K, 2024-2027</p></li> 
<li> <p> Exploiting Context in Multilingual Understanding and Generation, ARC Future Fellowship, 850K, 2020-2025 </p></li> 
<li> <p> Hierarchical Abstractions and Reasoning for Neuro-Symbolic Systems, 5.5M, 2023-2027 </p></li>
<li>Large-scale multimodal knowledge management: From organization and user modeling to fast contextual presentation, 4M, 2022-2025</li>
<li>Trustworthy Generative AI: Towards Safe and Aligned Foundation Models, 840K, 2024-2026</li>
<li>Dialogue Assistance for Negotiations in Cross-cultural Settings: A Neuro-Symbolic Computational Approach, 3.4M, 2022-2024</li>
<li>Learning to Learn and Adapt with Less Labels, 3.2M, 2019-2022</li>
<li>Continual Active Learning, EBAy Research, 200K, 2022-2023</li>
<li>Document-Wide Neural Machine Translation, EBay Research, 130K, 2020-2021</li>
<li>Effective Multi-Task Learning for Neural Machine Translation, Amazon Faculty Research Award, 113K, 2019-2020</li>
<li>Effective Learning of Document Neural Machine Translation, Google Faculty Research Award, 100K, 2019-2020</li>
<li>Document Machine Translation as Deep Structured Prediction, Google Faculty Research Award, 120K, 2018-2019</li>
<li>Explaining the outcomes of complex models, 430K, ARC DP, 2018-2022</li> 
<li>Learning Deep Semantics for Automatic Translation between Human Languages, ARC DP, 450K, 2015-2018</li>
</ul>
</div>

<div class="vspace"></div><h5>Recent Professional Services</h5>
<ul>
<li> <p> Senior Area Chair, Empirical Methods in Natural Language Processing (EMNLP), 2024</p></li>
<li> <p> Reviewer, Neural Information Processing Systems (NeurIPS), 2024 </p></li>
<li> <p> Reviewer, Association for Computational Linguistics (ACL), 2024 </p></li>
</ul>
</div>


<?php include("footer.htm"); ?>

